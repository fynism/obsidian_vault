## 1. 整体架构思路

项目实际分成两大子系统：

1. **人脸识别门禁（家长 / 学生身份核验）**

   - 基于 InsightFace（`buffalo_l`）做人脸检测 + 特征提取（512 维 embedding）。
   - 通过特征库（家长库 / 学生库）做相似度检索，匹配身份。
   - 在校门或图书馆门口由 `GateCheckProcessor` 驱动，`gate_check.py`/`script/test_gate_library.py` 是入口脚本。

2. **危险行为检测（校园安防事件）**

   - 基于 YOLOv8 系列模型做人形检测 / 姿态 / 暴力检测。
   - 加上一层规则 + 时序计数，实现“跌倒、攀爬、人群聚集、打架、危险物品”报警。
   - 由 `BehaviorEventProcessor` 驱动，`behavior_event.py` 和 `script/test_cam*_*.py` 是不同摄像头场景的入口。

   两部分共用的“后处理基建”：

- `EventDeduplicator`：按事件类型 + 时间窗口做去重（同一类型 60s 内只上报一次）。
- `MultiModalEventRecorder`：缓存视频帧，出事时回溯前若干秒、后若干秒，生成**截图 + 视频片段**。

------

## 2. 人脸识别门禁：算法原理

### 2.1 特征提取与增强

核心类：

- `FaceFeatureExtractor`
- `EnhancedFaceFeatureExtractor`
- `MaskDetector`
- `FeatureDatabaseManager`

**(1) FaceFeatureExtractor（基础特征提取）**

```
src/face_feature_extractor.py
```

- 用 InsightFace 的 `FaceAnalysis(name='buffalo_l')`：
  - 模型负责**检测人脸 + 对齐 + 提取 512 维 embedding**。
- 对每张图像：
  - 用 `self.app.get(img)` 获取所有人脸。
  - 若多张人脸，选择**面积最大的人脸**作为主脸。
  - 输出：
    - `embedding`：512 维特征向量
    - `bbox`：人脸框 `[x1, y1, x2, y2]`
    - `landmarks`：5 点关键点
    - `det_score`：检测分数
- 相似度计算：`compute_similarity` = **余弦相似度**。
- 是否同一人：`similarity >= 阈值(默认 0.35)`。

**(2) EnhancedFaceFeatureExtractor（增强 + 口罩感知）**

```
src/enhanced_face_extractor.py
```

在基础特征提取上叠加：

- 图像增强：通过 `ImageEnhancer.adaptive_enhance` 自动判断逆光/低照度等场景，对图像做增强后再做人脸检测。
  - 若增强图检测不到人脸，会回退到原图。
- 口罩检测：调用 `MaskDetector.detect_mask_by_landmarks`：
  - 利用 5 点关键点，截取鼻子—嘴巴区域作为“口罩区域”；
  - 分析：
    - HSV 颜色方差（口罩颜色通常更均匀，饱和度较低）
    - Canny 边缘密度（口罩有一定边缘结构）
    - 与眼部区域的亮度对比。
  - 综合得出 `has_mask` 与 `mask_confidence`。
- 阈值自适应：
  - 若检测到口罩，则通过 `get_mask_adaptive_threshold` 把匹配阈值从 0.35 **动态下调到约 0.25~0.20**（取决于置信度），补偿口罩遮挡带来的相似度下降。
- 对视频场景也有 `extract_features_from_image` 版本。

**(3) FeatureDatabaseManager（特征库管理）**

```
src/feature_database_manager.py
```

- 启动时：
  - 扫描 `faces_dir`（家长人脸库 / 学生人脸库），通过 `FaceFeatureExtractor` 抽取特征；
  - 把所有 `{filename: {embedding, bbox, det_score}}` 序列化到 `features.pkl`；
  - 有版本管理（`FeatureDBVersionManager`）与目录监控（`FaceDirectoryWatcher`），可以自动增量更新特征库。
- 检索流程：`search_face(query_image_path, threshold)`：
  - 用查询图做人脸特征提取，得到 `query_embedding`。
  - 遍历特征库，依次算余弦相似度。
  - 过滤掉 `< threshold` 的结果，按相似度降序排序，返回匹配列表。

### 2.2 门禁业务逻辑（家长/学生识别）

**GateCheckProcessor** (`src/main/gate_check_processor.py`)

- 输入：某个 gate 的待识别人脸图片目录（例如 `./datas/raw_parent_images/gate_library`）。

- 运行流程：

  1. 初始化 `FeatureDatabaseManager`，若特征库为空会自动对家长人脸库做一次构建。
  2. 对每张输入图：
     - 调用 `feature_manager.search_face()` 做人脸检索；
     - 取相似度最高的匹配，结合文件名规则提取 `parent_id`：
       - 支持 `parent_123.jpg` 或 `123_xxx.jpg` 等模式。
  3. 生成 `GateCheckRequest`：
     - `location`：摄像头 / 校门名称；
     - `parentId`：识别出的家长 ID（失败则 -1）。
  4. 可选：向后端 `POST /api/gate/check` 发送 JSON 请求。

  **多场景模拟脚本**

- `script/test_gate_library.py`：
  - 模拟“图书馆门口”校门，设置不同的 `input_dir` / `location`。
- 你也可以在别的校门配置多个 GateCheckProcessor，实现多点门禁。

------

## 3. 危险行为检测：算法原理

BehaviorEventProcessor 是一个**多检测器融合**的封装，内部用到了 4 个视觉算法模块：

- 跌倒：`FallDetector`
- 攀爬围栏：`ClimbingDetector`
- 人群聚集：`CrowdGatheringDetector`
- 打架/危险物品：`FightingDangerousItemDetector`

### 3.1 跌倒检测 (SICKNESS)

```
src/fall_detection.py
```

**模型：** YOLOv8 pose (`yolov8n-pose.pt`)，输出人体关键点和 bbox。

**核心特征：**

- 从 COCO 17 关键点中取：

  - 鼻子（NOSE）
  - 左/右髋部（LEFT_HIP, RIGHT_HIP）

- 计算：

  1. **头–髋垂直距离**：`head_to_hip_distance = |nose_y - hip_y|`

     - 与图像高度的比例 `head_to_hip_distance / image_height`，跌倒时人更“横着躺”，这个距离会变小。

  2. **人体框宽高比**：`aspect_ratio = bbox_width / bbox_height`

     - 躺倒时宽高比会增大（>1）。

     **规则判定：**

- 若某帧存在目标满足：
  - `head_to_hip_distance < head_to_hip_ratio * image_height`（默认 0.3）
  - `aspect_ratio > aspect_ratio_threshold`（默认 1.8）
  - 则认为该帧有 **“跌倒候选”**。
- 维护一个全局计数器 `_consecutive_fall_like_frames`：
  - 连续帧都有候选 → 计数 +1
  - 中间断掉 → 清零
- 当连续帧数 ≥ `min_consecutive_frames`（默认 3）时，`trigger_alert=True`，输出候选框列表给上层。

### 3.2 攀爬围栏检测 (CLIMBING)

```
src/climbing_detection.py
```

**模型：** YOLOv8 detection (`yolov8n.pt`)，用 COCO `person` 类（类别 ID 0）。

**空间规则（ROI）**：

- 在 `roi_zones.json` 中配置多个围栏区域的多边形顶点。
- 初始化时加载这些多边形：`Dict[str, np.ndarray]`。

**检测流程：**

- 对单帧：
  1. 用 YOLO 检出所有 `person` 框。
  2. 取每个框的中心点 `(cx, cy)`。
  3. 对所有 ROI 多边形，用 `cv2.pointPolygonTest` 判断点是否在多边形内部或边界。
  4. 在 ROI 内的目标，生成 `ClimbingEvent(bbox, score, roi_name)`。
- 时序逻辑：
  - 若当前帧有行人在任意 ROI 内 → `_frames_in_roi += 1`；
  - 否则 → `_frames_in_roi = 0`。
  - 若 `_frames_in_roi >= fps * min_stay_seconds`（默认 25fps * 2s ≈ 50 帧），触发 CLIMBING 报警。

### 3.3 人群聚集检测 (CROWD_GATHERING)

```
src/crowd_gathering_detection.py
```

**模型：** YOLOv8 detection (`yolov8n.pt`)，同样使用 person 类。

**空间聚类（DBSCAN）：**

- 从所有 person 框中取中心点坐标 `[(cx, cy), ...]`。

- 若人数 `< min_cluster_size`（默认 3）直接视为无聚集。

- 否则用 **DBSCAN 聚类**：

  - `eps = eps_pixels`（像素尺度的邻域半径，默认 150 像素，更适应大场景）。
  - `min_samples = min_cluster_size`。

- 对每个簇：

  - 计算簇中心点（平均坐标）和簇大小。
  - 只保留 **簇内人数 ≥ min_cluster_size** 的簇。

  **时序逻辑：**

- 如果当前帧存在至少一个满足条件的簇 → `_consecutive_crowd_frames += 1`；否则清零。
- 当 `_consecutive_crowd_frames >= min_frames`（`fps * min_duration_seconds`，默认 1 秒）时触发报警。

在 `BehaviorEventProcessor` 中，人群检测是**每帧都运行**（保证连续性），并通过 `EventDeduplicator` 做 60s 内去重，上报时会人为构造一个围绕簇中心的“虚拟 bbox”用于可视化。

### 3.4 打架/危险物品检测 (FIGHTING & DANGEROUS_ITEM)

```
src/violence_detection.py
```

**模型：** 自定义 YOLOv8 暴力 / 安防模型权重 `yolo_small_weights.pt`（Hugging Face 训练好后保存到本地）。

**类别映射策略：**

- 模型输出的是一些类别名，例如 `"violence"`, `"fight"`, `"knife"`, `"gun"` 等。
- 通过关键词匹配，将模型类别归并为两个逻辑事件类型：
  - `FIGHTING`：类名中包含 `fight`, `fighting`, `violence` 等。
  - `DANGEROUS_ITEM`：类名中包含 `weapon`, `knife`, `gun`, `pistol`, `rifle`, `firearm` 等。
- 每个检测框 → `SecurityEvent(event_type, label, bbox, score)`。

**时序计数：**

- 模型推理时设置 `conf >= conf_threshold`（默认 0.6）。
- 若当前帧至少有一个 SecurityEvent → `_consecutive_event_frames += 1`，否则清零。
- 连续帧数 ≥ `min_consecutive_frames`（默认 2） → `trigger_alert=True`。

`BehaviorEventProcessor` 收到报警后，会结合人脸识别（可选）标出 `student_id`，并调用多模态记录器生成截图/视频。

------

## 4. 事件去重与多模态告警

### 4.1 EventDeduplicator（时间去重）

```
src/event_processing.py
```

- 内部维护：`event_type -> last_timestamp`。
- `should_emit(event_type, bbox, timestamp)`：
  - 若同种事件距上次上报时间 < `dedup_seconds`（默认 60s），则**丢弃**。
  - 否则更新 last_timestamp 并允许上报。
- 当前实现只按“类型 + 时间”去重，不按空间网格做精细划分，简单但足够应对单摄像头场景。

### 4.2 MultiModalEventRecorder（截图+视频片段）

同文件中 `MultiModalEventRecorder`：

- 内部维护一个按时间排序的帧缓冲 `deque[BufferedFrame(timestamp, frame)]`：

  - `add_frame(frame, timestamp)`：不断把帧入队，并删除时间跨度超过 `max_buffer_seconds` 的旧帧（默认 30s）。

- `save_event_media(frame, event_timestamp, events, base_name)`：

  1. `save_event_screenshot`：

     - 用 `_draw_events_on_frame` 在当前帧上画 bbox + 文本（事件类型 / 标签 / 置信度）。
     - 保存为 `base_name.jpg`。

  2. `save_event_video_clip`：

     - 选取 `[event_timestamp - pre_seconds, event_timestamp + post_seconds]` 时间窗内的所有缓冲帧（默认前 10 秒、后 10 秒）。
     - 若帧数不足 `min_frames`（默认 15），尝试扩大窗口；仍不足则放弃生成视频。
     - 根据相邻帧时间差估计 fps（不足时用默认 fps 25）。
     - 把第一帧加上标注，其余帧原样写入 `base_name.mp4`。

     在 `BehaviorEventProcessor` 中，每帧都会调用 `recorder.add_frame`，一旦有事件触发，就能立即回溯生成完整的**事件时间线视频**。

------

## 5. 多摄像头 / 多场景的组织方式

`script/` 目录的三个脚本本质上只是在变换“位置 + 数据目录”：

- `test_cam1_library.py`：

  - 位置：图书馆
  - 视频目录：`./datas/raw_risky_videos/cam1`
  - 创建一套 `BehaviorEventProcessor`，location="图书馆"，并将事件发到统一后端。

- `test_cam2_north_gate.py`：

  - 位置：北一门
  - 视频目录：`./datas/raw_risky_videos/cam2`
  - 算法层完全相同，只是换了数据源和 location 标签。

- `test_gate_library.py`：

  - 位置：图书馆门口
  - 图片目录：`./datas/raw_parent_images/gate_library`
  - 走人脸门禁流程（GateCheckProcessor）。

  因此，从算法角度看：

- 底层检测算法是统一的一套；

- 不同摄像头 / 校门通过不同配置（路径 + 位置字符串）实例化不同的处理器实例，逻辑完全复用。

  

  

# 问题

## 一、项目整体相关

**Q1：你这个项目整体是干什么的？解决什么问题？**
A：
这个项目是一个校园安防的视觉算法模块，主要做两件事：
一是**人脸识别门禁**，在校门或图书馆门口识别家长/学生的身份；
二是**危险行为检测**，比如跌倒、攀爬围栏、人群聚集、打架和危险物品，然后生成报警事件、截图和视频片段，方便上层系统统一处理。

------

**Q2：你们的系统整体架构是怎么样的？**
A：
底层是几个独立的视觉算法模块：人脸识别、跌倒检测、攀爬检测、人群聚集检测、暴力/危险物品检测等。
上面有两个处理器：`GateCheckProcessor` 专门负责门禁核验，`BehaviorEventProcessor` 专门负责从视频里检测各种行为事件。
最上层还有一个事件处理模块，用来做**重复报警的去重**，以及生成**带标注的截图和短视频**，最后再由测试脚本或 HTTP 接口把结果发给后端。

------

## 二、人脸识别与口罩处理

**Q3：你的人脸识别是用什么模型？怎么判断是不是同一个人？**
A：
人脸部分用的是 InsightFace 提供的 `buffalo_l` 模型，它会先检测人脸、对齐，然后输出一个 512 维的人脸特征向量。
我对两张脸的相似度用的是**余弦相似度**，相似度大于一个阈值（默认 0.35）就认为是同一个人，这个阈值是结合经验和实际试验定的。

------

**Q4：家长/学生身份是怎么从特征库里查出来的？**
A：
我会先对待识别的图片提取一个 512 维的 embedding，然后和特征库里每一条人脸特征做余弦相似度计算。
把相似度超过阈值的结果筛出来，再按相似度从高到低排序，取第一个作为匹配结果；家长 ID 或学生 ID 是通过图片文件名解析出来的，比如 `parent_123.jpg` 或 `student_1001.jpg`。

------

**Q5：你是怎么处理戴口罩这种情况的？阈值为什么要变？**
A：
戴口罩会遮挡一大块脸，相似度整体会下降，如果还用正常场景的阈值，容易把“同一人”判成“不同人”。
所以我先通过口罩检测模块判断 `has_mask` 和 `mask_confidence`，如果检测到戴口罩，就用一个函数把阈值从 0.35 动态调低到大概 0.20–0.25 之间。
这样既能提高口罩场景下的通过率，又不会把阈值降得太夸张，从而控制误识率。

------

**Q6：具体怎么判断一个人是不是戴口罩？（非特别细节版）**
A：
我利用 InsightFace 输出的 5 个关键点（两眼、鼻子、两嘴角），定位出“鼻子到嘴巴”这块区域作为候选四边形。
在这个区域上，我看三个方面：

- 颜色是不是比较均匀、饱和度较低（像蓝色/白色口罩）；
- 边缘的密度是否在一个合理范围（有一些轮廓，但不特别杂乱）；
- 和上方眼睛区域的亮度差是否比较大。
  综合这些特征算出两个分数，超过一定阈值就认为戴了口罩，并给出一个置信度。

------

## 三、行为检测模块（跌倒 / 攀爬 / 人群 / 暴力）

**Q7：跌倒检测的基本原理是什么？**
A：
跌倒用的是 YOLOv8 的姿态模型，它会给出人体的 17 个关键点和一个人体框。
我主要利用“鼻子”和“髋部”这两个部位：计算头–髋在竖直方向的距离和画面高度的比例，同时计算人体框的宽高比。
如果一个人从直立变成几乎横躺，头–髋竖直距离会变小、宽高比会变大；我用这两个条件加上“连续多帧都满足”来判断疑似跌倒。

------

**Q8：为什么要看“连续多帧”而不是一帧就报警？**
A：
单帧很容易被一些偶然姿态误伤，比如弯腰捡东西、刚好一个动作被截到。
用连续多帧（比如 3 帧）可以过滤掉这种瞬间动作，只有持续保持“躺倒姿态”的才触发报警，这样误报会少很多。

------

**Q9：攀爬围栏检测是怎么做的？**
A：
攀爬用的是 YOLOv8 检测模型，先检测出所有行人，再计算每个行人框的中心点。
我在一个 JSON 配置里预先画好了围栏区域的多边形（像 ROI），然后用 OpenCV 的 `pointPolygonTest` 判断这个中心点是不是落在围栏区域内部。
如果连续几秒钟每一帧都有行人在这些 ROI 里，就认为存在攀爬或者在围栏附近停留过久，触发 CLIMBING 事件。

------

**Q10：人群聚集检测里的“聚集”是怎么定义的？为什么要用聚类？**
A：
先用 YOLOv8 检出所有行人，取每个行人的 bbox 中心点当成一个“人”的坐标。
然后用 DBSCAN 聚类，把距离比较近的点分成一簇一簇；如果某个簇人数超过一定阈值（比如 3 人），并且连续若干帧都存在这种大簇，就认为发生了人群聚集。
用 DBSCAN 的好处是不用我自己去指定簇的数量，而且它对噪声点（散落的几个人）有比较好的过滤能力。

------

**Q11：暴力/危险物品检测是怎么区分类别的？**
A：
这块用的是一个预训练的 YOLOv8 安防/暴力模型，它本身输出的是一些具体类别，比如 `violence`, `fight`, `knife`, `gun` 等。
在系统里我做了一层简单映射：凡是类名里包含 `fight` 或 `violence` 的都归为 FIGHTING；包含 `weapon`, `knife`, `gun` 等关键词的都归为 DANGEROUS_ITEM。
这样上层业务只需要处理两种逻辑事件，而不用关心具体的模型类别细节。

------

## 四、事件去重与多模态输出

**Q12：你怎么避免同一事件一直报警（防止刷屏）？**
A：
我实现了一个很简单的时间去重模块 `EventDeduplicator`，只按“事件类型 + 时间”做去重。
对每种事件类型，我会记录上一次上报的时间戳，如果新的事件在 60 秒内再次出现，就认为是重复，不再上报。
这个策略虽然不算特别精细，但对单摄像头场景已经能有效防止同一种事件在短时间内刷屏。

------

**Q13：为什么要保存截图和视频片段？怎么截取事件前后的视频？**
A：
截图可以直观地看到当时的画面，视频片段可以让老师或平台回放事件发生前后的全过程，这在安防场景里很重要。
我在主循环中对每一帧都写入一个环形缓冲区，里面保存最近几十秒的帧和时间戳。
当检测到事件时，就从缓冲区里截取“事件前 N 秒 + 后 M 秒”的帧，生成一个 MP4 视频，同时在当前帧上画好框和文字后保存成一张截图。

------

## 五、工程 & 实验层面

**Q14：为什么选择 YOLOv8 和 InsightFace，而不是别的模型？**
A：
YOLOv8 在目标检测和姿态估计上性能比较均衡，实现简单，Python 生态也比较成熟，适合课程项目。
InsightFace 在人脸识别领域比较成熟，官方就提供了 `buffalo_l` 这类开箱即用的高质量模型，省去了我们自己训练的成本。
这一套组合能让我们把精力放在业务逻辑和工程设计上，而不是从零训练模型。

------

**Q15：你觉得你这个项目最大的特点或者亮点是什么？**
A：
我觉得有两个：
一是把“人脸识别门禁”和“行为检测”放在同一个 SDK 里，很多基础模块可以复用，比如特征库管理、事件处理等。
二是做了一层比较完整的**事件后处理**，包括时间去重、多模态输出、支持多摄像头脚本，这些让它更接近实际工程系统，而不是单个 demo。

------

**Q16：如果老师问你这个系统有什么局限性，你会怎么回答？**
A：
局限性主要有几个：

- 模型完全依赖公开预训练权重，没有针对校园场景做专项训练，某些复杂场景下准确率可能不够高；
- 目前的去重只按时间，不按空间网格，多个摄像头联合部署时可能需要更细粒度的规则；
- 口罩检测是启发式规则，光照特别极端或有特殊花纹的口罩时效果可能不稳定。
  这些都可以作为后续改进方向，比如引入更强的模型或增加更细的业务规则。